<?xml version="1.0" encoding="utf-8" ?>
<configuration>

  <configSections>
    <section name = "abot" type="Abot.Core.AbotConfigurationSectionHandler, Abot"/>
  </configSections>

  <abot>
    <crawlBehavior
      maxConcurrentThreads = "1"
      maxPagesToCrawl="10"
      maxPagesToCrawlPerDomain="0"
      maxPageSizeInBytes="0"
      userAgentString="Mozilla/5.0 (Windows NT 6.3; Trident/7.0; rv:11.0) like Gecko"
      crawlTimeoutSeconds="0"
      downloadableContentTypes="text/html, text/plain"
      isUriRecrawlingEnabled="false"
      isExternalPageCrawlingEnabled="false"
      isExternalPageLinksCrawlingEnabled="false"
      httpServicePointConnectionLimit="200"
      httpRequestTimeoutInSeconds="15"
      httpRequestMaxAutoRedirects="7"
      isHttpRequestAutoRedirectsEnabled="true"
      isHttpRequestAutomaticDecompressionEnabled="false"
      isSendingCookiesEnabled="false"
      isSslCertificateValidationEnabled="false"
      isRespectUrlNamedAnchorOrHashbangEnabled="false"
      minAvailableMemoryRequiredInMb="0"
      maxMemoryUsageInMb="0"
      maxMemoryUsageCacheTimeInSeconds="0"
      maxCrawlDepth="100"
      maxLinksPerPage="1000"
      isForcedLinkParsingEnabled="false"
      maxRetryCount="0"
      minRetryDelayInMilliseconds="0"
      />
    <authorization
      isAlwaysLogin = "false"
      loginUser=""
      loginPassword="" />
    <politeness
      isRespectRobotsDotTextEnabled = "false"
      isRespectMetaRobotsNoFollowEnabled="false"
      isRespectAnchorRelNoFollowEnabled="false"
      isIgnoreRobotsDotTextIfRootDisallowedEnabled="false"
      robotsDotTextUserAgentString="abot"
      maxRobotsDotTextCrawlDelayInSeconds="5"
      minCrawlDelayPerDomainMilliSeconds="1000"/>
  </abot>

</configuration>